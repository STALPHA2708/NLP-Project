{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-01T12:37:51.778363Z",
     "iopub.status.busy": "2024-12-01T12:37:51.777988Z",
     "iopub.status.idle": "2024-12-01T12:37:53.093466Z",
     "shell.execute_reply": "2024-12-01T12:37:53.092342Z",
     "shell.execute_reply.started": "2024-12-01T12:37:51.778331Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T12:37:58.951127Z",
     "iopub.status.busy": "2024-12-01T12:37:58.950664Z",
     "iopub.status.idle": "2024-12-01T12:37:59.168021Z",
     "shell.execute_reply": "2024-12-01T12:37:59.167049Z",
     "shell.execute_reply.started": "2024-12-01T12:37:58.951099Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"sentences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T12:38:03.861668Z",
     "iopub.status.busy": "2024-12-01T12:38:03.861289Z",
     "iopub.status.idle": "2024-12-01T12:38:03.898388Z",
     "shell.execute_reply": "2024-12-01T12:38:03.897370Z",
     "shell.execute_reply.started": "2024-12-01T12:38:03.861642Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>darija</th>\n",
       "      <th>eng</th>\n",
       "      <th>darija_ar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>homa mkhbbyin chi haja, ana mti99en!</td>\n",
       "      <td>They're hiding something, I'm sure!</td>\n",
       "      <td>هوما مخبّيين شي حاجة, أنا متيقّن!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bayna homa tay7awlo ib9aw mbrrdin.</td>\n",
       "      <td>It's obvious they're trying to keep their cool.</td>\n",
       "      <td>باينا هوما تايحاولو إبقاو مبرّدين.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>loTilat mabaynach fihom mori7in bzzaf.</td>\n",
       "      <td>the hotels don't seem very comfortable.</td>\n",
       "      <td>لوطيلات مابايناش فيهوم موريحين بزّاف.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ghaliban ghayjrriw 3lih mn lkhdma!</td>\n",
       "      <td>he is probably about to be laid off by head of...</td>\n",
       "      <td>غاليبان غايجرّيو عليه من لخدما!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tab3an rah mkta2eb!</td>\n",
       "      <td>of course he's depressive!</td>\n",
       "      <td>طابعان راه مكتاءب!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   darija  \\\n",
       "0    homa mkhbbyin chi haja, ana mti99en!   \n",
       "1      bayna homa tay7awlo ib9aw mbrrdin.   \n",
       "2  loTilat mabaynach fihom mori7in bzzaf.   \n",
       "3      ghaliban ghayjrriw 3lih mn lkhdma!   \n",
       "4                     Tab3an rah mkta2eb!   \n",
       "\n",
       "                                                 eng  \\\n",
       "0                They're hiding something, I'm sure!   \n",
       "1    It's obvious they're trying to keep their cool.   \n",
       "2            the hotels don't seem very comfortable.   \n",
       "3  he is probably about to be laid off by head of...   \n",
       "4                         of course he's depressive!   \n",
       "\n",
       "                               darija_ar  \n",
       "0      هوما مخبّيين شي حاجة, أنا متيقّن!  \n",
       "1     باينا هوما تايحاولو إبقاو مبرّدين.  \n",
       "2  لوطيلات مابايناش فيهوم موريحين بزّاف.  \n",
       "3        غاليبان غايجرّيو عليه من لخدما!  \n",
       "4                     طابعان راه مكتاءب!  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.dropna()\n",
    "\n",
    "# Display the first few rows to understand the structure\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T12:38:06.276727Z",
     "iopub.status.busy": "2024-12-01T12:38:06.276403Z",
     "iopub.status.idle": "2024-12-01T12:38:06.292506Z",
     "shell.execute_reply": "2024-12-01T12:38:06.291219Z",
     "shell.execute_reply.started": "2024-12-01T12:38:06.276698Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>darija</th>\n",
       "      <th>eng</th>\n",
       "      <th>darija_ar</th>\n",
       "      <th>english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>homa mkhbbyin chi haja, ana mti99en!</td>\n",
       "      <td>They're hiding something, I'm sure!</td>\n",
       "      <td>هوما مخبّيين شي حاجة, أنا متيقّن!</td>\n",
       "      <td>They're hiding something, I'm sure!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bayna homa tay7awlo ib9aw mbrrdin.</td>\n",
       "      <td>It's obvious they're trying to keep their cool.</td>\n",
       "      <td>باينا هوما تايحاولو إبقاو مبرّدين.</td>\n",
       "      <td>It's obvious they're trying to keep their cool.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>loTilat mabaynach fihom mori7in bzzaf.</td>\n",
       "      <td>the hotels don't seem very comfortable.</td>\n",
       "      <td>لوطيلات مابايناش فيهوم موريحين بزّاف.</td>\n",
       "      <td>the hotels don't seem very comfortable.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ghaliban ghayjrriw 3lih mn lkhdma!</td>\n",
       "      <td>he is probably about to be laid off by head of...</td>\n",
       "      <td>غاليبان غايجرّيو عليه من لخدما!</td>\n",
       "      <td>he is probably about to be laid off by head of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tab3an rah mkta2eb!</td>\n",
       "      <td>of course he's depressive!</td>\n",
       "      <td>طابعان راه مكتاءب!</td>\n",
       "      <td>of course he's depressive!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12742</th>\n",
       "      <td>ra deja chet imta 5aykoun lfilm</td>\n",
       "      <td>i've already checked the showtimes</td>\n",
       "      <td>را دجا شت إمتا خايكون لفيلم</td>\n",
       "      <td>i've already checked the showtimes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12743</th>\n",
       "      <td>nhar l7ed 5aydar flil m3etel</td>\n",
       "      <td>there's a late night screening on saturday</td>\n",
       "      <td>نهار لحد خايدار فليل معتل</td>\n",
       "      <td>there's a late night screening on saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12744</th>\n",
       "      <td>ana kangoul nmchiw nt3echaw 9bel</td>\n",
       "      <td>i'm thinking we could grab some dinner before ...</td>\n",
       "      <td>أنا كانڭول نمشيو نتعشاو قبل</td>\n",
       "      <td>i'm thinking we could grab some dinner before ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12745</th>\n",
       "      <td>blan nadi</td>\n",
       "      <td>that's a great plan</td>\n",
       "      <td>بلان نادي</td>\n",
       "      <td>that's a great plan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12746</th>\n",
       "      <td>film ou 3cha a7ssen combo</td>\n",
       "      <td>dinner and a movie, the ultimate combo</td>\n",
       "      <td>فيلم و عشا أحسّن كومبو</td>\n",
       "      <td>dinner and a movie, the ultimate combo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12743 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       darija  \\\n",
       "0        homa mkhbbyin chi haja, ana mti99en!   \n",
       "1          bayna homa tay7awlo ib9aw mbrrdin.   \n",
       "2      loTilat mabaynach fihom mori7in bzzaf.   \n",
       "3          ghaliban ghayjrriw 3lih mn lkhdma!   \n",
       "4                         Tab3an rah mkta2eb!   \n",
       "...                                       ...   \n",
       "12742         ra deja chet imta 5aykoun lfilm   \n",
       "12743            nhar l7ed 5aydar flil m3etel   \n",
       "12744        ana kangoul nmchiw nt3echaw 9bel   \n",
       "12745                               blan nadi   \n",
       "12746               film ou 3cha a7ssen combo   \n",
       "\n",
       "                                                     eng  \\\n",
       "0                    They're hiding something, I'm sure!   \n",
       "1        It's obvious they're trying to keep their cool.   \n",
       "2                the hotels don't seem very comfortable.   \n",
       "3      he is probably about to be laid off by head of...   \n",
       "4                             of course he's depressive!   \n",
       "...                                                  ...   \n",
       "12742                 i've already checked the showtimes   \n",
       "12743         there's a late night screening on saturday   \n",
       "12744  i'm thinking we could grab some dinner before ...   \n",
       "12745                                that's a great plan   \n",
       "12746             dinner and a movie, the ultimate combo   \n",
       "\n",
       "                                   darija_ar  \\\n",
       "0          هوما مخبّيين شي حاجة, أنا متيقّن!   \n",
       "1         باينا هوما تايحاولو إبقاو مبرّدين.   \n",
       "2      لوطيلات مابايناش فيهوم موريحين بزّاف.   \n",
       "3            غاليبان غايجرّيو عليه من لخدما!   \n",
       "4                         طابعان راه مكتاءب!   \n",
       "...                                      ...   \n",
       "12742            را دجا شت إمتا خايكون لفيلم   \n",
       "12743              نهار لحد خايدار فليل معتل   \n",
       "12744            أنا كانڭول نمشيو نتعشاو قبل   \n",
       "12745                              بلان نادي   \n",
       "12746                 فيلم و عشا أحسّن كومبو   \n",
       "\n",
       "                                                 english  \n",
       "0                    They're hiding something, I'm sure!  \n",
       "1        It's obvious they're trying to keep their cool.  \n",
       "2                the hotels don't seem very comfortable.  \n",
       "3      he is probably about to be laid off by head of...  \n",
       "4                             of course he's depressive!  \n",
       "...                                                  ...  \n",
       "12742                 i've already checked the showtimes  \n",
       "12743         there's a late night screening on saturday  \n",
       "12744  i'm thinking we could grab some dinner before ...  \n",
       "12745                                that's a great plan  \n",
       "12746             dinner and a movie, the ultimate combo  \n",
       "\n",
       "[12743 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['english'] = data['eng']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T12:38:10.535221Z",
     "iopub.status.busy": "2024-12-01T12:38:10.534882Z",
     "iopub.status.idle": "2024-12-01T12:38:11.156425Z",
     "shell.execute_reply": "2024-12-01T12:38:11.155381Z",
     "shell.execute_reply.started": "2024-12-01T12:38:10.535195Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming the CSV has two columns: 'English' and 'Darija'\n",
    "english_sentences = data['english'].tolist()\n",
    "darija_sentences = data['darija'].tolist()\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "train_eng, temp_eng, train_dar, temp_dar = train_test_split(english_sentences, darija_sentences, test_size=0.2)\n",
    "val_eng, test_eng, val_dar, test_dar = train_test_split(temp_eng, temp_dar, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T12:38:17.231530Z",
     "iopub.status.busy": "2024-12-01T12:38:17.231177Z",
     "iopub.status.idle": "2024-12-01T12:38:32.269121Z",
     "shell.execute_reply": "2024-12-01T12:38:32.267927Z",
     "shell.execute_reply.started": "2024-12-01T12:38:17.231506Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenizer_eng = Tokenizer()\n",
    "tokenizer_eng.fit_on_texts(train_eng)\n",
    "train_eng_seq = tokenizer_eng.texts_to_sequences(train_eng)\n",
    "val_eng_seq = tokenizer_eng.texts_to_sequences(val_eng)\n",
    "\n",
    "tokenizer_dar = Tokenizer()\n",
    "tokenizer_dar.fit_on_texts(train_dar)\n",
    "train_dar_seq = tokenizer_dar.texts_to_sequences(train_dar)\n",
    "val_dar_seq = tokenizer_dar.texts_to_sequences(val_dar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T12:38:35.020055Z",
     "iopub.status.busy": "2024-12-01T12:38:35.018240Z",
     "iopub.status.idle": "2024-12-01T12:38:37.000772Z",
     "shell.execute_reply": "2024-12-01T12:38:36.999805Z",
     "shell.execute_reply.started": "2024-12-01T12:38:35.020010Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\w'\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_7980\\1562620273.py:7: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  data['english'] = data['english'].str.lower().str.replace('[^\\w\\s]', '', regex=True)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_7980\\1562620273.py:8: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  data['darija'] = data['darija'].str.lower().str.replace('[^\\w\\s]', '', regex=True)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "# Convert the data to lowercase and remove any special characters\n",
    "data['english'] = data['english'].str.lower().str.replace('[^\\w\\s]', '', regex=True)\n",
    "data['darija'] = data['darija'].str.lower().str.replace('[^\\w\\s]', '', regex=True)\n",
    "\n",
    "# Add special tokens to the sentences\n",
    "data['english'] = data['english'].apply(lambda x: 'starttoken ' + x + ' endtoken')\n",
    "data['darija'] = data['darija'].apply(lambda x: 'starttoken ' + x + ' endtoken')\n",
    "\n",
    "# Initialize tokenizers and add special tokens explicitly\n",
    "english_tokenizer = Tokenizer(filters='')\n",
    "darija_tokenizer = Tokenizer(filters='')\n",
    "\n",
    "# Manually add the special tokens\n",
    "english_tokenizer.fit_on_texts(['starttoken', 'endtoken'])\n",
    "darija_tokenizer.fit_on_texts(['starttoken', 'endtoken'])\n",
    "\n",
    "# Fit the tokenizers on the actual text data\n",
    "english_tokenizer.fit_on_texts(data['english'])\n",
    "darija_tokenizer.fit_on_texts(data['darija'])\n",
    "\n",
    "# Convert text to sequences\n",
    "english_sequences = english_tokenizer.texts_to_sequences(data['english'])\n",
    "darija_sequences = darija_tokenizer.texts_to_sequences(data['darija'])\n",
    "\n",
    "\n",
    "\n",
    "# Pad the sequences\n",
    "max_eng = [len(seq) for seq in english_sequences]\n",
    "max_english_length = 0\n",
    "for i in max_eng:\n",
    "    if i>max_english_length:\n",
    "        max_english_length=i\n",
    "\n",
    "max_dar = [len(seq) for seq in darija_sequences]\n",
    "max_darija_length = 0\n",
    "for i in max_dar:\n",
    "    if i>max_darija_length:\n",
    "        max_darija_length=i\n",
    "        \n",
    "english_sequences = pad_sequences(english_sequences, maxlen=max_english_length, padding='post')\n",
    "darija_sequences = pad_sequences(darija_sequences, maxlen=max_darija_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T12:38:44.297631Z",
     "iopub.status.busy": "2024-12-01T12:38:44.297241Z",
     "iopub.status.idle": "2024-12-01T12:38:44.319149Z",
     "shell.execute_reply": "2024-12-01T12:38:44.317988Z",
     "shell.execute_reply.started": "2024-12-01T12:38:44.297605Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(english_sequences, darija_sequences, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T12:38:53.540109Z",
     "iopub.status.busy": "2024-12-01T12:38:53.539700Z",
     "iopub.status.idle": "2024-12-01T12:38:53.964558Z",
     "shell.execute_reply": "2024-12-01T12:38:53.963617Z",
     "shell.execute_reply.started": "2024-12-01T12:38:53.540083Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,417,472</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,857,408</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],       │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]        │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15068</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,872,476</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │  \u001b[38;5;34m1,417,472\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │  \u001b[38;5;34m3,857,408\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),     │    \u001b[38;5;34m525,312\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m256\u001b[0m), │    \u001b[38;5;34m525,312\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],       │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]        │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m15068\u001b[0m) │  \u001b[38;5;34m3,872,476\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,197,980</span> (38.90 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,197,980\u001b[0m (38.90 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,197,980</span> (38.90 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,197,980\u001b[0m (38.90 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "\n",
    "# Define the encoder\n",
    "encoder_inputs = Input(shape=(max_english_length,))\n",
    "encoder_embedding = Embedding(len(english_tokenizer.word_index) + 1, 256, mask_zero=True)(encoder_inputs)\n",
    "encoder_lstm, state_h, state_c = LSTM(256, return_state=True)(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Define the decoder\n",
    "decoder_inputs = Input(shape=(max_darija_length,))\n",
    "decoder_embedding = Embedding(len(darija_tokenizer.word_index) + 1, 256, mask_zero=True)(decoder_inputs)\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(len(darija_tokenizer.word_index) + 1, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T12:39:09.355754Z",
     "iopub.status.busy": "2024-12-01T12:39:09.355363Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 1s/step - loss: 7.3753 - val_loss: 5.9137\n",
      "Epoch 2/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 1s/step - loss: 5.5381 - val_loss: 5.9135\n",
      "Epoch 3/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 1s/step - loss: 5.2961 - val_loss: 5.8921\n",
      "Epoch 4/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 1s/step - loss: 5.0660 - val_loss: 5.9029\n",
      "Epoch 5/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 1s/step - loss: 4.8533 - val_loss: 5.9255\n",
      "Epoch 6/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 1s/step - loss: 4.6695 - val_loss: 5.9392\n",
      "Epoch 7/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 1s/step - loss: 4.4372 - val_loss: 5.9650\n",
      "Epoch 8/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 1s/step - loss: 4.2116 - val_loss: 6.0061\n",
      "Epoch 9/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 1s/step - loss: 3.9837 - val_loss: 6.0548\n",
      "Epoch 10/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 1s/step - loss: 3.7737 - val_loss: 6.1035\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x25f85a17f80>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare target data by shifting the sequences\n",
    "y_train_shifted = np.zeros_like(y_train)\n",
    "y_train_shifted[:, :-1] = y_train[:, 1:]\n",
    "\n",
    "y_val_shifted = np.zeros_like(y_val)\n",
    "y_val_shifted[:, :-1] = y_val[:, 1:]\n",
    "\n",
    "# Train the model\n",
    "model.fit([X_train, y_train], y_train_shifted, validation_data=([X_val, y_val], y_val_shifted), epochs=10, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T15:43:16.946479Z",
     "iopub.status.busy": "2024-12-01T15:43:16.945557Z",
     "iopub.status.idle": "2024-12-01T15:43:17.253679Z",
     "shell.execute_reply": "2024-12-01T15:43:17.252029Z",
     "shell.execute_reply.started": "2024-12-01T15:43:16.946429Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 180ms/step - loss: 6.1806\n",
      "Validation Loss: 6.101609230041504\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "loss = model.evaluate([X_val, y_val], y_val_shifted)\n",
    "print(f'Validation Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T22:21:51.701719Z",
     "iopub.status.busy": "2024-06-18T22:21:51.701358Z",
     "iopub.status.idle": "2024-06-18T22:21:52.28389Z",
     "shell.execute_reply": "2024-06-18T22:21:52.282929Z",
     "shell.execute_reply.started": "2024-06-18T22:21:51.70169Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\w'\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_7980\\1957419781.py:3: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  sentence = 'starttoken ' + sentence.lower().replace('[^\\w\\s]', '') + ' endtoken'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "wach kayn chi 7aja\n"
     ]
    }
   ],
   "source": [
    "def translate(sentence):\n",
    "    # Preprocess the sentence\n",
    "    sentence = 'starttoken ' + sentence.lower().replace('[^\\w\\s]', '') + ' endtoken'\n",
    "    sequence = english_tokenizer.texts_to_sequences([sentence])\n",
    "    sequence = pad_sequences(sequence, maxlen=max_english_length, padding='post')\n",
    "\n",
    "    # Encode the sentence\n",
    "    states = encoder_model.predict(sequence)\n",
    "    \n",
    "    # Initialize the target sequence with the start token\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = darija_tokenizer.word_index['starttoken']\n",
    "    \n",
    "    translated_sentence = []\n",
    "    \n",
    "    for _ in range(max_darija_length):\n",
    "        # Predict the next word\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states)\n",
    "        \n",
    "        # Get the highest probability word index\n",
    "        word_id = np.argmax(output_tokens[0, -1, :])\n",
    "        word = darija_tokenizer.index_word.get(word_id, '')\n",
    "        \n",
    "        if word == 'endtoken':\n",
    "            break\n",
    "        \n",
    "        translated_sentence.append(word)\n",
    "        \n",
    "        # Update the target sequence and states\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = word_id\n",
    "        states = [h, c]\n",
    "\n",
    "    return ' '.join(translated_sentence)\n",
    "\n",
    "# Define the encoder model for prediction\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Define the decoder model for prediction\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_embedding, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Translate a sample sentence\n",
    "print(translate('how i can help you?').replace('starttoken ',''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the entire model\n",
    "model.save('translation_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save English tokenizer\n",
    "with open('english_tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(english_tokenizer, f)\n",
    "\n",
    "# Save Darija tokenizer\n",
    "with open('darija_tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(darija_tokenizer, f)\n",
    "\n",
    "# Save maximum sequence lengths\n",
    "with open('max_seq_lengths.pkl', 'wb') as f:\n",
    "    pickle.dump({'max_english_length': max_english_length, 'max_darija_length': max_darija_length}, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5193871,
     "sourceId": 8724031,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
